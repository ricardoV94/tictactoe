{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# From game theory to general information seeking\n",
    "\n",
    "## Intro\n",
    "\n",
    "Minimax assumes the worst case scenario, where the opponent is optimal. This simplifies search to a single worst case scenario world model, but can miss rewards when facing weaker opponents (e.g., via traps).\n",
    "\n",
    "If one were to know with certainty the model of the opponent (and that opponent model of oneself), that could be used to model adversary choices via a recursive minimax search. The simplest cases involve:\n",
    "\n",
    "1. `X` assumes `O` plays randomly, in which case the evaluation converges to a simple dynamic programming search (or expectiminimax without adversary nodes).\n",
    "\n",
    "2. `X` knows that `O` assumes `X` plays randomly, in which case `X` can simulate how `O` would evaluate each position and play against that. This allows for deterministic traps. It assumes `O` would never realize `X` is not actually playing at random.\n",
    "\n",
    "Curiosity question: If both assume the other assumes each one plays randomly, it would converge back to the original minimax?\n",
    "\n",
    "3. `X` knows that `O` uses a given heuristic instead of recursive minimax search (mixed models where an adversary may use minimax for `n` iterations followed by heuristic evaluation can also be considered)\n",
    "\n",
    "In each case it should be possible (provided the game is flexible enoguh) for `X` to increase it's expected payoff by deviating from the optimal policy, specially in cases where it starts at a disadvantage.\n",
    "\n",
    "**Section 1** provides simple demos for these 3 scenarios, with references to more extensive works that have been published in the literature.\n",
    "\n",
    "***\n",
    "\n",
    "**Section 2** exhamines more complicated scenarios, where `X` does not know for sure what the model of `O` is, but might be able to infer this based on their choices. In enough complex games this would in theory motivate \"adversary-testing\" moves that sacrifice some efficiency / incur some risk to obtain extra information about the adversary.\n",
    "\n",
    "This provides an example of artifical-intelligence information seeking in a context where the agent has a perfect (deterministic or stochastic) model of the world, and it is obvious (for the programmer at least) what cues are valuable and when they should be collected / evaluated.\n",
    "\n",
    "Literature review is needed to assess what has been done in this field already\n",
    "\n",
    "***\n",
    "\n",
    "**Section 3** explores model-based inference and information seeking in less constrained environments... NEED A GOOD LEARNING ENVIRONMENT FOR THIS!\n",
    "\n",
    "Literature review is needed to asses what has been done in this field already\n",
    "\n",
    "***\n",
    "\n",
    "**(Possibly unrelated)**\n",
    "\n",
    "Genetic algorithm for meta-learning / exploration-exploitation based on simple learning models / information content cues\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}